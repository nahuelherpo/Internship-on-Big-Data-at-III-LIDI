[[package]]
name = "py4j"
version = "0.10.9.5"
description = "Enables Python programs to dynamically access arbitrary Java objects"
category = "main"
optional = false
python-versions = "*"

[[package]]
name = "pyspark"
version = "3.3.1"
description = "Apache Spark Python API"
category = "main"
optional = false
python-versions = ">=3.7"

[package.dependencies]
py4j = "0.10.9.5"

[package.extras]
ml = ["numpy (>=1.15)"]
mllib = ["numpy (>=1.15)"]
pandas_on_spark = ["numpy (>=1.15)", "pandas (>=1.0.5)", "pyarrow (>=1.0.0)"]
sql = ["pandas (>=1.0.5)", "pyarrow (>=1.0.0)"]

[metadata]
lock-version = "1.1"
python-versions = "^3.10"
content-hash = "8b463a46c8d7bec3082b6adb1d634a52a4f45589fb64a1e44aeb3f7ed8b5df73"

[metadata.files]
py4j = [
    {file = "py4j-0.10.9.5-py2.py3-none-any.whl", hash = "sha256:52d171a6a2b031d8a5d1de6efe451cf4f5baff1a2819aabc3741c8406539ba04"},
    {file = "py4j-0.10.9.5.tar.gz", hash = "sha256:276a4a3c5a2154df1860ef3303a927460e02e97b047dc0a47c1c3fb8cce34db6"},
]
pyspark = [
    {file = "pyspark-3.3.1.tar.gz", hash = "sha256:e99fa7de92be406884bfd831c32b9306a3a99de44cfc39a2eefb6ed07445d5fa"},
]
